{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnVZO8H9PhsY8jmJIHFqv8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuanSunGo/default_prediction_in_large_scale/blob/main/src/Keras_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Set UP"
      ],
      "metadata": {
        "id": "Ib9FvGuy5jWF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRWWPwA34trV",
        "outputId": "cc5260be-81f8-47c9-9962-e3732b15a1ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔️ Libraries Imported!\n"
          ]
        }
      ],
      "source": [
        "# fundamental packages \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt \n",
        "from matplotlib.ticker import MaxNLocator \n",
        "import random\n",
        "import math \n",
        "#! pip install colorama\n",
        "from colorama import Fore, Back, Style \n",
        "import gc # garbage collector\n",
        "\n",
        "# sklearn packages \n",
        "from sklearn.model_selection import StratifiedGroupKFold, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# keras tensorflow packages \n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler,EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Add, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model \n",
        "\n",
        "INFERENCE = True\n",
        "\n",
        "print('✔️ Libraries Imported!')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training history\n",
        "def plot_history(history, *, n_epochs = None, plot_lr = False, title = None, bottom = None, top = None):\n",
        "  \"\"\"\n",
        "  Function of plotting the last n_epochs of the training history,\n",
        "  plots loss and optionally val_loss and lr. \n",
        "  \"\"\"\n",
        "  plt.figure(figsize = (15, 6))\n",
        "  # here the `history` is the model result defined in `fit_model` function\n",
        "  from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0) \n",
        "\n",
        "  # plot training and validation losses \n",
        "  plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label = 'Training loss')\n",
        "\n",
        "  pass "
      ],
      "metadata": {
        "id": "75IXxWpVEffz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading and Preprocessing the training data"
      ],
      "metadata": {
        "id": "usOXRHPXIE62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I read the parquet file as well that has already been denoised. \n",
        "\n",
        "I'm about to create few groups of features: \n",
        "- Averaged selected features over all statements of a customer\n",
        "- Minimum and maximum of the selected features over all statements of a customer \n",
        "- Selected features taken from the last statement of a customer \n",
        "\n",
        "I'm going to use __*one hot encoding*__ for the categorical features, and fill in all the missing values with 0 as Keras not taking Nan as the input. \n",
        "\n",
        "In practice, `.iloc[mask_array, columns]` needs much less RAM than the `groupby` method, and for saving more memory for  deleting the index of the training dataframe. "
      ],
      "metadata": {
        "id": "st7aqfeYJXEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%% time \n",
        "features_avg = ['B_11', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', \n",
        "                'B_20', 'B_28', 'B_29', 'B_3', 'B_33', 'B_36', 'B_37', 'B_4', 'B_42', \n",
        "                'B_5', 'B_8', 'B_9', 'D_102', 'D_103', 'D_105', 'D_111', 'D_112', 'D_113', \n",
        "                'D_115', 'D_118', 'D_119', 'D_121', 'D_124', 'D_128', 'D_129', 'D_131', \n",
        "                'D_132', 'D_133', 'D_139', 'D_140', 'D_141', 'D_143', 'D_144', 'D_145', \n",
        "                'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', \n",
        "                'D_49', 'D_50', 'D_51', 'D_52', 'D_56', 'D_58', 'D_62', 'D_70', 'D_71', \n",
        "                'D_72', 'D_74', 'D_75', 'D_79', 'D_81', 'D_83', 'D_84', 'D_88', 'D_91', \n",
        "                'P_2', 'P_3', 'R_1', 'R_10', 'R_11', 'R_13', 'R_18', 'R_19', 'R_2', 'R_26', \n",
        "                'R_27', 'R_28', 'R_3', 'S_11', 'S_12', 'S_22', 'S_23', 'S_24', 'S_26', \n",
        "                'S_27', 'S_5', 'S_7', 'S_8', ]\n",
        "features_min = ['B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_19', 'B_2', 'B_20', 'B_22', \n",
        "                'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_33', 'B_36', 'B_4', 'B_42', \n",
        "                'B_5', 'B_9', 'D_102', 'D_103', 'D_107', 'D_109', 'D_110', 'D_111', \n",
        "                'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_128', \n",
        "                'D_129', 'D_132', 'D_133', 'D_139', 'D_140', 'D_141', 'D_143', 'D_144', \n",
        "                'D_145', 'D_39', 'D_41', 'D_42', 'D_45', 'D_46', 'D_48', 'D_50', 'D_51', \n",
        "                'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', \n",
        "                'D_71', 'D_74', 'D_75', 'D_78', 'D_79', 'D_81', 'D_83', 'D_84', 'D_86', \n",
        "                'D_88', 'D_96', 'P_2', 'P_3', 'P_4', 'R_1', 'R_11', 'R_13', 'R_17', 'R_19', \n",
        "                'R_2', 'R_27', 'R_28', 'R_4', 'R_5', 'R_8', 'S_11', 'S_12', 'S_23', 'S_25', \n",
        "                'S_3', 'S_5', 'S_7', 'S_9', ]\n",
        "features_max = ['B_1', 'B_11', 'B_13', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', \n",
        "                'B_22', 'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_31', 'B_33', 'B_36', \n",
        "                'B_4', 'B_42', 'B_5', 'B_7', 'B_9', 'D_102', 'D_103', 'D_105', 'D_109', \n",
        "                'D_110', 'D_112', 'D_113', 'D_115', 'D_121', 'D_124', 'D_128', 'D_129', \n",
        "                'D_131', 'D_139', 'D_141', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', \n",
        "                'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_50', 'D_51', 'D_52', \n",
        "                'D_53', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', 'D_72', 'D_74', \n",
        "                'D_75', 'D_79', 'D_81', 'D_83', 'D_84', 'D_88', 'D_89', 'P_2', 'P_3', \n",
        "                'R_1', 'R_10', 'R_11', 'R_26', 'R_28', 'R_3', 'R_4', 'R_5', 'R_7', 'R_8', \n",
        "                'S_11', 'S_12', 'S_23', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_7', 'S_8', ]\n",
        "features_last = ['B_1', 'B_11', 'B_12', 'B_13', 'B_14', 'B_16', 'B_18', 'B_19', 'B_2', \n",
        "                 'B_20', 'B_21', 'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30', 'B_31', \n",
        "                 'B_33', 'B_36', 'B_37', 'B_38', 'B_39', 'B_4', 'B_40', 'B_42', 'B_5', \n",
        "                 'B_8', 'B_9', 'D_102', 'D_105', 'D_106', 'D_107', 'D_108', 'D_110', \n",
        "                 'D_111', 'D_112', 'D_113', 'D_114', 'D_115', 'D_116', 'D_117', 'D_118', \n",
        "                 'D_119', 'D_120', 'D_121', 'D_124', 'D_126', 'D_128', 'D_129', 'D_131', \n",
        "                 'D_132', 'D_133', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', \n",
        "                 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', \n",
        "                 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_55', \n",
        "                 'D_56', 'D_59', 'D_60', 'D_62', 'D_63', 'D_64', 'D_66', 'D_68', 'D_70', \n",
        "                 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_77', 'D_78', 'D_81', 'D_82', \n",
        "                 'D_83', 'D_84', 'D_88', 'D_89', 'D_91', 'D_94', 'D_96', 'P_2', 'P_3', \n",
        "                 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_16', 'R_17', 'R_18', \n",
        "                 'R_19', 'R_25', 'R_28', 'R_3', 'R_4', 'R_5', 'R_8', 'S_11', 'S_12', \n",
        "                 'S_23', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_7', 'S_8', 'S_9', ]\n",
        "features_categorical = ['B_30_last', 'B_38_last', 'D_114_last', 'D_116_last',\n",
        "                        'D_117_last', 'D_120_last', 'D_126_last',\n",
        "                        'D_63_last', 'D_64_last', 'D_66_last', 'D_68_last']"
      ],
      "metadata": {
        "id": "4o9zmfeCIHu5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}