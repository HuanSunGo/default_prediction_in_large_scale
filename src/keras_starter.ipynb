{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib9FvGuy5jWF"
      },
      "source": [
        "## Environment Set UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRWWPwA34trV",
        "outputId": "cc5260be-81f8-47c9-9962-e3732b15a1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✔️ Libraries Imported!\n"
          ]
        }
      ],
      "source": [
        "# fundamental packages \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt \n",
        "from matplotlib.ticker import MaxNLocator \n",
        "import random\n",
        "import datetime\n",
        "import math \n",
        "#! pip install colorama\n",
        "from colorama import Fore, Back, Style \n",
        "import gc # garbage collector\n",
        "\n",
        "# sklearn packages \n",
        "from sklearn.model_selection import StratifiedGroupKFold, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# keras tensorflow packages \n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler,EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Input, InputLayer, Add, Concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import plot_model \n",
        "\n",
        "INFERENCE = True\n",
        "\n",
        "print('✔️ Libraries Imported!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75IXxWpVEffz"
      },
      "outputs": [],
      "source": [
        "# plot training history\n",
        "def plot_history(history, *, n_epochs = None, plot_lr = False, title = None, bottom = None, top = None):\n",
        "  \"\"\"\n",
        "  Function of plotting the last n_epochs of the training history,\n",
        "  plots loss and optionally val_loss and lr. \n",
        "  \"\"\"\n",
        "  plt.figure(figsize = (15, 6))\n",
        "  # here the `history` is the model result defined in `fit_model` function\n",
        "  from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0) \n",
        "\n",
        "  # plot training and validation losses \n",
        "  plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label = 'Training loss')\n",
        "\n",
        "  pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usOXRHPXIE62"
      },
      "source": [
        "## Reading and Preprocessing the training data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st7aqfeYJXEP"
      },
      "source": [
        "Here I read the parquet file as well that has already been denoised. \n",
        "\n",
        "I'm about to create few groups of features: \n",
        "- Averaged selected features over all statements of a customer\n",
        "- Minimum and maximum of the selected features over all statements of a customer \n",
        "- Selected features taken from the last statement of a customer \n",
        "\n",
        "I'm going to use __*one hot encoding*__ for the categorical features, and fill in all the missing values with 0 as Keras not taking Nan as the input. \n",
        "\n",
        "In practice, `.iloc[mask_array, columns]` needs much less RAM than the `groupby` method, and for saving more memory for  deleting the index of the training dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o9zmfeCIHu5"
      },
      "outputs": [],
      "source": [
        "features_avg = ['B_11', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', \n",
        "                'B_20', 'B_28', 'B_29', 'B_3', 'B_33', 'B_36', 'B_37', 'B_4', 'B_42', \n",
        "                'B_5', 'B_8', 'B_9', 'D_102', 'D_103', 'D_105', 'D_111', 'D_112', 'D_113', \n",
        "                'D_115', 'D_118', 'D_119', 'D_121', 'D_124', 'D_128', 'D_129', 'D_131', \n",
        "                'D_132', 'D_133', 'D_139', 'D_140', 'D_141', 'D_143', 'D_144', 'D_145', \n",
        "                'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', \n",
        "                'D_49', 'D_50', 'D_51', 'D_52', 'D_56', 'D_58', 'D_62', 'D_70', 'D_71', \n",
        "                'D_72', 'D_74', 'D_75', 'D_79', 'D_81', 'D_83', 'D_84', 'D_88', 'D_91', \n",
        "                'P_2', 'P_3', 'R_1', 'R_10', 'R_11', 'R_13', 'R_18', 'R_19', 'R_2', 'R_26', \n",
        "                'R_27', 'R_28', 'R_3', 'S_11', 'S_12', 'S_22', 'S_23', 'S_24', 'S_26', \n",
        "                'S_27', 'S_5', 'S_7', 'S_8', ]\n",
        "features_min = ['B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_19', 'B_2', 'B_20', 'B_22', \n",
        "                'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_33', 'B_36', 'B_4', 'B_42', \n",
        "                'B_5', 'B_9', 'D_102', 'D_103', 'D_107', 'D_109', 'D_110', 'D_111', \n",
        "                'D_112', 'D_113', 'D_115', 'D_118', 'D_119', 'D_121', 'D_122', 'D_128', \n",
        "                'D_129', 'D_132', 'D_133', 'D_139', 'D_140', 'D_141', 'D_143', 'D_144', \n",
        "                'D_145', 'D_39', 'D_41', 'D_42', 'D_45', 'D_46', 'D_48', 'D_50', 'D_51', \n",
        "                'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', \n",
        "                'D_71', 'D_74', 'D_75', 'D_78', 'D_79', 'D_81', 'D_83', 'D_84', 'D_86', \n",
        "                'D_88', 'D_96', 'P_2', 'P_3', 'P_4', 'R_1', 'R_11', 'R_13', 'R_17', 'R_19', \n",
        "                'R_2', 'R_27', 'R_28', 'R_4', 'R_5', 'R_8', 'S_11', 'S_12', 'S_23', 'S_25', \n",
        "                'S_3', 'S_5', 'S_7', 'S_9', ]\n",
        "features_max = ['B_1', 'B_11', 'B_13', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', \n",
        "                'B_22', 'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_31', 'B_33', 'B_36', \n",
        "                'B_4', 'B_42', 'B_5', 'B_7', 'B_9', 'D_102', 'D_103', 'D_105', 'D_109', \n",
        "                'D_110', 'D_112', 'D_113', 'D_115', 'D_121', 'D_124', 'D_128', 'D_129', \n",
        "                'D_131', 'D_139', 'D_141', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', \n",
        "                'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_50', 'D_51', 'D_52', \n",
        "                'D_53', 'D_56', 'D_58', 'D_59', 'D_60', 'D_62', 'D_70', 'D_72', 'D_74', \n",
        "                'D_75', 'D_79', 'D_81', 'D_83', 'D_84', 'D_88', 'D_89', 'P_2', 'P_3', \n",
        "                'R_1', 'R_10', 'R_11', 'R_26', 'R_28', 'R_3', 'R_4', 'R_5', 'R_7', 'R_8', \n",
        "                'S_11', 'S_12', 'S_23', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_7', 'S_8', ]\n",
        "features_last = ['B_1', 'B_11', 'B_12', 'B_13', 'B_14', 'B_16', 'B_18', 'B_19', 'B_2', \n",
        "                 'B_20', 'B_21', 'B_24', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30', 'B_31', \n",
        "                 'B_33', 'B_36', 'B_37', 'B_38', 'B_39', 'B_4', 'B_40', 'B_42', 'B_5', \n",
        "                 'B_8', 'B_9', 'D_102', 'D_105', 'D_106', 'D_107', 'D_108', 'D_110', \n",
        "                 'D_111', 'D_112', 'D_113', 'D_114', 'D_115', 'D_116', 'D_117', 'D_118', \n",
        "                 'D_119', 'D_120', 'D_121', 'D_124', 'D_126', 'D_128', 'D_129', 'D_131', \n",
        "                 'D_132', 'D_133', 'D_137', 'D_138', 'D_139', 'D_140', 'D_141', 'D_142', \n",
        "                 'D_143', 'D_144', 'D_145', 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', \n",
        "                 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_55', \n",
        "                 'D_56', 'D_59', 'D_60', 'D_62', 'D_63', 'D_64', 'D_66', 'D_68', 'D_70', \n",
        "                 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_77', 'D_78', 'D_81', 'D_82', \n",
        "                 'D_83', 'D_84', 'D_88', 'D_89', 'D_91', 'D_94', 'D_96', 'P_2', 'P_3', \n",
        "                 'P_4', 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_16', 'R_17', 'R_18', \n",
        "                 'R_19', 'R_25', 'R_28', 'R_3', 'R_4', 'R_5', 'R_8', 'S_11', 'S_12', \n",
        "                 'S_23', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_7', 'S_8', 'S_9', ]\n",
        "features_categorical = ['B_30_last', 'B_38_last', 'D_114_last', 'D_116_last',\n",
        "                        'D_117_last', 'D_120_last', 'D_126_last',\n",
        "                        'D_63_last', 'D_64_last', 'D_66_last', 'D_68_last']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vcJitxfaDPt",
        "outputId": "5b893363-a3d8-4ac5-e879-7d9def52a719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:10: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed avg train\n",
            "Computed max train\n",
            "Computed min train\n",
            "Computed last train\n",
            "Computed categorical train\n",
            "train shape: (458913, 435)\n",
            "Read test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<timed exec>:10: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed avg test\n",
            "Computed max test\n",
            "Computed min test\n",
            "Computed last test\n",
            "Computed categorical test\n",
            "test shape: (924621, 407)\n",
            "CPU times: total: 6min 31s\n",
            "Wall time: 5min 36s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "for i in ['train','test'] if INFERENCE else ['train']:\n",
        "    df = pd. read_parquet(f'../data/{i}.parquet') \n",
        "    cid = pd.Categorical(df.pop('customer_ID'), ordered = True)\n",
        "    last = (cid != np.roll(cid, -1)) # mask for last statement of every customer \n",
        "    if 'target' in df.columns:\n",
        "        df.drop(columns = ['target'], inplace = True)\n",
        "    print('Read', i)\n",
        "    gc.collect() # performs a blocking garbage collection of all generations\n",
        "    \n",
        "    df_avg = (df\n",
        "                .groupby(cid)\n",
        "                .mean()[features_avg]\n",
        "                .rename(columns = {f: f\"{f}_avg\" for f in features_avg})\n",
        "                )\n",
        "    print('Computed avg', i )\n",
        "    gc.collect()\n",
        "\n",
        "    df_max = (df\n",
        "                .groupby(cid)\n",
        "                .max()[features_max]\n",
        "                .rename(columns = {f: f\"{f}_max\" for f in features_max})\n",
        "                )\n",
        "    print('Computed max', i)\n",
        "    gc.collect()\n",
        "\n",
        "    df_min = (df\n",
        "              .groupby(cid)\n",
        "              .min()[features_min]\n",
        "              .rename(columns={f: f\"{f}_min\" for f in features_min})\n",
        "             )\n",
        "    print('Computed min', i)\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    df_last = (df.loc[last, features_last]\n",
        "               .rename(columns={f: f\"{f}_last\" for f in features_last})\n",
        "               .set_index(np.asarray(cid[last]))\n",
        "              )\n",
        "    df = None # we no longer need the original data\n",
        "    print('Computed last', i)\n",
        "\n",
        "    df_categorical = df_last[features_categorical].astype(object)\n",
        "    features_not_cat = [f for f in df_last.columns if f not in features_categorical]\n",
        "    if i == 'train':\n",
        "        ohe = OneHotEncoder(drop = 'first', sparse = False, \n",
        "                            dtype = np.float32, handle_unknown = 'ignore')\n",
        "        ohe.fit(df_categorical)\n",
        "        # where is this from? \n",
        "        with open('ohe.pickle', 'wb') as f:pickle.dump(ohe, f)\n",
        "        df_categorical = pd.DataFrame(ohe.transform(df_categorical).astype(np.float16),\n",
        "                                    index = df_categorical.index).rename(columns = str)\n",
        "    print('Computed categorical', i)\n",
        "\n",
        "    df = pd.concat([df_last[features_not_cat], df_categorical, df_avg, df_min, df_max], axis=1)\n",
        "    \n",
        "    # Impute missing values\n",
        "    df.fillna(value=0, inplace=True)\n",
        "    \n",
        "    del df_avg, df_max, df_min, df_last, df_categorical, cid, last, features_not_cat\n",
        "\n",
        "    print(f\"{i} shape: {df.shape}\")\n",
        "    if i == 'train': \n",
        "        # free the memory \n",
        "        df.reset_index(drop = True, inplace = True) # free the memory \n",
        "        df.to_feather('train_processed.ftr')\n",
        "        df = None \n",
        "        gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gkwkJ8naDPu",
        "outputId": "9f261b44-06b9-4aed-90b3-bd856a48e37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target shape:(458913, 2)\n"
          ]
        }
      ],
      "source": [
        "train = pd.read_feather('train_processed.ftr')\n",
        "target = pd.read_csv('../data/train_labels.csv')\n",
        "print(f'target shape:{target.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNsHymadaDPv"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjrOk8IWaDPw"
      },
      "source": [
        "For the neural network model, my rule of thumb is to start with one hidden layer, then gradually add more hidden layers, and when the network seems to overfit or diverges, I'll insert a Dropout and/or a connection which will skips two layers. <br><br>\n",
        "The final model has four hidden layers, and is enriched by a skip connection and a Dropout layer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy_ZpaxqaDPw"
      },
      "outputs": [],
      "source": [
        "LR_START = 0.01\n",
        "features = [f for f in train.columns if f != 'target' and f != 'customer_ID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssfRfn8haDPw"
      },
      "outputs": [],
      "source": [
        "def keras_model(n_inputs = len(features)):\n",
        "    \"\"\"\n",
        "    Function for a sequential neural network with a skip connection. \n",
        "    \n",
        "    Returns a complicated instance of tensorflows.keras.models.Model\n",
        "    \"\"\"\n",
        "\n",
        "    activation = 'swish' # why use swish as the activation function here? \n",
        "    reg = 4e-4 # what is `reg` here?\n",
        "    inputs = Input(shape = (n_inputs,))\n",
        "\n",
        "    # build neural network layers \n",
        "    x0 = Dense (256, kernel_regularizer = tf.keras.regularizers.l2(reg),\n",
        "            activation = activation,\n",
        "            )(inputs)\n",
        "    x = Dense (64, kernel_regularizer = tf.keras.regularizers.l2(reg),\n",
        "            activation = activation,\n",
        "            )(x0)\n",
        "    x = Concatenate()([ x , x0])\n",
        "    x = Dropout(0.1)(x) # how is the dropout work, and does the sequence matters? \n",
        "    x = Dense(16, kernel_regularizer = tf.keras.regularizers.l2(reg),\n",
        "            activation = activation,\n",
        "            )(x)\n",
        "    x = Dense (1, #kernel_regularizer = tf.keras.regularizers.l2(reg),\n",
        "            activation = 'sigmoid',\n",
        "            )(x) # meaning of the numbers in dense layer? \n",
        "    \n",
        "    model = Model(inputs, x)\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate= LR_START),\n",
        "                loss = tf.keras.losses.BinaryCrossentropy()) # rewind the crossentropy\n",
        "    \n",
        "    return model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k44Z941raDPx",
        "outputId": "873ae45d-7cb3-4ef2-89d4-091b7fd6e5ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "plot_model(keras_model(), \n",
        "        show_layer_names = False,\n",
        "        show_shapes = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rse6vFVXaDPx"
      },
      "source": [
        "### Cross - Validation \n",
        "I'll use a standar validation loop, in which we scale the data and train the model. Since the dataset is imbalance, we'll use a StratifiedKFold because of it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PN-hYkaaDPy"
      },
      "outputs": [],
      "source": [
        "# set up the parameters \n",
        "ONLY_FIRST_FOLD = False \n",
        "EPOCHS_EXPONENTIALDECAY = 100 \n",
        "VERBOSE = 0 # set to 0 for less output, or to 2 for more output \n",
        "LR_END = 1e-5 # learning rate at the end of training \n",
        "CYCLES = 1 \n",
        "EPOCHS = 200 # how to choose the epochs? \n",
        "DIAGRAMS = True \n",
        "USE_PLATEAU = False # set to True for early stopping, or to False for exponential learning rate decay \n",
        "BATCH_SIZE = 2048 \n",
        "\n",
        "np.random.seed(1)\n",
        "random.seed(1) \n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM1J7ZanaDPy"
      },
      "outputs": [],
      "source": [
        "def exponential_decay(epoch): \n",
        "    \"\"\"\n",
        "    The helper function for the learning rate's exponentially decay.\n",
        "    -------------------\n",
        "    v decays from e^a to 1 in every cycle\n",
        "    w decays from 1 to 0 in every cycle\n",
        "    epoch == 0                  -> w = 1 (first epoch of cycle)\n",
        "    epoch == epochs_per_cycle-1 -> w = 0 (last epoch of cycle)\n",
        "    higher a                    -> decay starts with a steeper decline\n",
        "    \"\"\"\n",
        "    a = 3\n",
        "    epochs_per_cycle = EPOCHS // CYCLES\n",
        "    epoch_in_cycle = epoch % epochs_per_cycle\n",
        "    if epochs_per_cycle > 1:\n",
        "        v = math.exp(a * (1 - epoch_in_cycle / (epochs_per_cycle-1)))\n",
        "        w = (v - 1) / (math.exp(a) - 1)\n",
        "    else:\n",
        "        w = 1\n",
        "    return w * LR_START + (1 - w) * LR_END    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-rS6yn9aDPy"
      },
      "outputs": [],
      "source": [
        "def fit_model ( X_tr, y_tr, X_va = None, y_va = None, fold = 0, run = 0):\n",
        "    \"\"\"\n",
        "    Scale the data, fit a model, plot the training history and optionally validate the model.\n",
        "    \n",
        "    Saves a trained instance of tensorflow.keras.models.Model.\n",
        "    \n",
        "    As a side effect, updates y_va_pred, history_list, y_pred_list and score_list.\n",
        "    \"\"\"\n",
        "\n",
        "    global y_va_pred \n",
        "    gc.collect()\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_tr = scaler.fit_transform(X_tr)\n",
        "\n",
        "    if X_va is not None: \n",
        "        X_va = scaler.transform(X_va)\n",
        "        validation_data = (X_va, y_va)\n",
        "    else:\n",
        "        validation_data = None \n",
        "    \n",
        "    #define the learning rate schedule and EarlyStopping \n",
        "    if USE_PLATEAU and X_va is not None: # use early stopping \n",
        "        epochs = EPOCHS \n",
        "        lr = ReduceLROnPlateau(monitor = 'val_loss', # there's a question asking about the implementation of this question \n",
        "                            factor = 0.7, \n",
        "                            patience = 4, \n",
        "                            verbose = VERBOSE\n",
        "\n",
        "        )\n",
        "        es = EarlyStopping(monitor = 'val_loss',\n",
        "                        patience = 12,\n",
        "                        verbose = 1,\n",
        "                        mode = 'min',\n",
        "                        restore_best_weights = True)\n",
        "        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN]\n",
        "    \n",
        "    else: \n",
        "        #use exponential learning rate decay rather than early stopping \n",
        "        epochs = EPOCHS_EXPONENTIALDECAY\n",
        "        lr = LearningRateScheduler(exponential_decay, verbose = 0)\n",
        "        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n",
        "\n",
        "    #construct and compile the model \n",
        "    model = keras_model(X_tr.shape[1])\n",
        "\n",
        "    #train the model \n",
        "    history = model.fit(X_tr, y_tr,\n",
        "                        validation_data = validation_data,\n",
        "                        epochs = epochs,\n",
        "                        verbose = VERBOSE,\n",
        "                        batch_size = BATCH_SIZE,\n",
        "                        shuffle = True,\n",
        "                        callbacks = callbacks)\n",
        "    del X_tr, y_tr\n",
        "\n",
        "    with open(f'scaler_{fold}.pickle','wb') as f:pickle.dump(scaler,f)\n",
        "    model.save(f'model_{fold}')\n",
        "    history_list.append(history.history)\n",
        "    callbacks, es, lr, history = None, None, None, None\n",
        "\n",
        "    if X_va is None: \n",
        "        print(f'Training loss:{history_list[-1]['loss'][-1]:.4f}')\n",
        "    else: \n",
        "        pass\n",
        "\n",
        "    pass \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit ('3.9.0')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "58c21a2341242efc0953e9856321acb0a34bb47bd9f06e4bbe32894437b3eb6a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}